import asyncio
import logging
import json
import os
import asyncpg
import redis.asyncio as redis
from datetime import datetime

# Setup Logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("TimescaleArchiver")

# Config
REDIS_URL = os.getenv("REDIS_URL", "redis://localhost:6379/0")
DB_HOST = os.getenv("DB_HOST", "localhost")
DB_PORT = os.getenv("DB_PORT", "5432")
DB_USER = os.getenv("DB_USER", "postgres")
DB_PASSWORD = os.getenv("DB_PASSWORD", "password")
DB_NAME = os.getenv("DB_NAME", "stockval")

BATCH_SIZE = 100
FLUSH_INTERVAL = 5  # seconds

class TimescaleArchiver:
    """RedisÏóêÏÑú Îç∞Ïù¥ÌÑ∞Î•º Íµ¨ÎèÖÌïòÏó¨ TimescaleDBÏóê ÏãúÍ≥ÑÏó¥Î°ú Ï†ÄÏû•ÌïòÎäî ÏïÑÏπ¥Ïù¥Î≤Ñ"""
    def __init__(self):
        self.redis = None
        self.db_pool = None
        self.batch = []
        self.running = True

    async def _record_db_success(self, count: int):
        """
        ISSUE-035: Record successful DB ingestion for monitoring
        Updates Redis key with latest success timestamp and count
        """
        try:
            now = datetime.now().isoformat()
            await self.redis.set("archiver:last_db_success", now)
            await self.redis.set("archiver:last_flush_count", count)
            await self.redis.set("archiver:last_flush_time", now)
        except Exception as e:
            logger.warning(f"Failed to record DB success metric: {e}")

    async def init_db(self):
        """
        [ISSUE-036 Revision] 
        Í∏∞Ï°¥ ÌïòÎìúÏΩîÎî©Îêú DDLÏùÑ Ï†úÍ±∞ÌïòÍ≥† ÎßàÏù¥Í∑∏Î†àÏù¥ÏÖò ÎèÑÍµ¨(migrate.sh)Î•º ÌÜµÌï¥ Í¥ÄÎ¶¨ÌïòÎèÑÎ°ù ÏúÑÏûÑÌï©ÎãàÎã§.
        Ïó¨Í∏∞ÏÑúÎäî ÌïÑÏàò ÌÖåÏù¥Î∏î Ï°¥Ïû¨ Ïó¨Î∂ÄÎßå Í≤ÄÏ¶ùÌï©ÎãàÎã§.
        """
        conn = await asyncpg.connect(
            user=DB_USER, password=DB_PASSWORD, database=DB_NAME, host=DB_HOST, port=DB_PORT
        )
        try:
            # ÌïÑÏàò ÌÖåÏù¥Î∏î Ï°¥Ïû¨ Ïó¨Î∂Ä ÌôïÏù∏ (SSoT: migrations/)
            required_tables = ['market_ticks', 'market_orderbook', 'system_metrics']
            for table in required_tables:
                exists = await conn.fetchval(
                    "SELECT EXISTS (SELECT FROM information_schema.tables WHERE table_name = $1)",
                    table
                )
                if not exists:
                    logger.error(f"CRITICAL: Table '{table}' not found. Please run 'make migrate-up' first.")
                    # Ïö¥ÏòÅ ÌôòÍ≤ΩÏóêÏÑúÎäî Ï¶âÏãú Ï§ëÎã®ÌïòÏó¨ Ï†ïÌï©ÏÑ± ÌõºÏÜê Î∞©ÏßÄ
                    raise RuntimeError(f"Database schema incomplete: table '{table}' missing.")
            
            logger.info("Database schema verification completed (SSoT: Migrations).")
        finally:
            await conn.close()

    async def start(self):
        # 1. Initialize DB Pool
        self.db_pool = await asyncpg.create_pool(
            user=DB_USER, password=DB_PASSWORD, database=DB_NAME, host=DB_HOST, port=DB_PORT
        )
        await self.init_db()
        logger.info("‚úÖ Database pool initialized.")

        # 2. Redis Connection Loop
        while self.running:
            try:
                self.redis = await redis.from_url(REDIS_URL, encoding="utf-8", decode_responses=True)
                await self.redis.ping()
                logger.info(f"‚úÖ Connected to Redis: {REDIS_URL}")

                pubsub = self.redis.pubsub()
                await pubsub.psubscribe("ticker.*", "tick:*", "orderbook.*", "system.*")
                logger.info("üì° Subscribed to: ticker.*, tick:*, orderbook.*, system.*")

                # Flush Task
                flush_task = asyncio.create_task(self.periodic_flush())

                try:
                    async for message in pubsub.listen():
                        msg_type = message["type"]
                        if msg_type == "pmessage":
                            await self.handle_pmessage(message)
                        elif msg_type == "message":
                            await self.handle_message(message)
                
                except redis.ConnectionError as e:
                    logger.error(f"Redis Connection Lost: {e}. Reconnecting...")
                except Exception as e:
                    logger.error(f"Unexpected error in listen loop: {e}")
                finally:
                    flush_task.cancel()
                    await self.redis.close()
                    logger.info("Redis connection closed. Retrying in 5s...")
                    await asyncio.sleep(5)
            
            except Exception as e:
                logger.error(f"Failed to connect to Redis: {e}. Retrying in 5s...")
                await asyncio.sleep(5)

    async def handle_pmessage(self, message):
        try:
            channel = message["channel"]
            data = json.loads(message["data"])
            
            # Handle both ticker.* (KIS) and tick:* (Kiwoom/Standard)
            if channel.startswith("ticker.") or channel.startswith("tick:"):
                # Extract market
                if channel.startswith("tick:"):
                    try:
                        market = channel.split(':')[1].upper()
                    except:
                        market = "KR"
                else:
                    market = channel.split('.')[-1].upper()
                
                # Ticks
                ts = datetime.fromisoformat(data['timestamp']) if 'timestamp' in data else datetime.now()
                source = data.get('source', 'KIS')
                broker = data.get('broker')
                broker_time = datetime.fromisoformat(data['broker_time']) if data.get('broker_time') else None
                received_time = datetime.fromisoformat(data['received_time']) if data.get('received_time') else datetime.now()
                seq_no = data.get('sequence_number')
                
                row = (
                    ts, 
                    data['symbol'], 
                    float(data['price']), 
                    float(data.get('volume', 0)), 
                    float(data.get('change', 0)),
                    broker,
                    broker_time,
                    received_time,
                    seq_no,
                    source
                )
                self.batch.append(row)
                
                if len(self.batch) >= BATCH_SIZE:
                    await self.flush()
            
            elif channel.startswith("orderbook."):
                await self.save_orderbook(data)
                
            elif channel == "system.metrics":
                await self.save_system_metrics(data)
                
        except Exception as e:
            logger.error(f"Parse/Queue Error (pattern): {e}")

    async def handle_message(self, message):
        try:
            channel = message["channel"]
            data = json.loads(message["data"])
            
            if channel == "market_orderbook":
                await self.save_orderbook(data)
            elif channel == "system.metrics": # Direct message fallback
                await self.save_system_metrics(data)
                
        except Exception as e:
            logger.error(f"Parse/Queue Error (direct): {e}")

    async def save_system_metrics(self, data):
        """ÏãúÏä§ÌÖú Î©îÌä∏Î¶≠ Ï†ÄÏû• (Generic)"""
        async with self.db_pool.acquire() as conn:
            try:
                ts = datetime.fromisoformat(data['timestamp'])
                
                # Check if data is legacy dict format (cpu, mem, disk) or new generic format (type, value, meta)
                if 'cpu' in data and 'mem' in data:
                     # Handle Legacy (Host Metrics) -> Convert to rows
                     rows = [
                         (ts, 'cpu', float(data['cpu']), None),
                         (ts, 'memory', float(data['mem']), None)
                     ]
                     if 'disk' in data:
                         rows.append((ts, 'disk', float(data['disk']), None))
                     
                     await conn.executemany("INSERT INTO system_metrics (time, metric_name, value, labels) VALUES ($1, $2, $3, $4)", rows)
                
                else:
                    # New Generic Format
                    m_type = data.get('type')
                    val = float(data.get('value'))
                    labels = json.dumps(data.get('meta')) if data.get('meta') else None
                    
                    await conn.execute("INSERT INTO system_metrics (time, metric_name, value, labels) VALUES ($1, $2, $3, $4)", ts, m_type, val, labels)

            except Exception as e:
                logger.error(f"System Metric Save Error: {e}")

    async def save_orderbook(self, data):
        """Ìò∏Í∞Ä Ïä§ÎÉÖÏÉ∑ Îç∞Ïù¥ÌÑ∞Î•º DBÏóê Ï†ÄÏû• (10 Depth Support)"""
        async with self.db_pool.acquire() as conn:
            try:
                ts = datetime.fromisoformat(data['timestamp'])
                source = data.get('source', 'KIS')
                
                # Base row: [time, symbol, source]
                row = [ts, data['symbol'], source]
                
                # Add Asks 1~10
                asks = data.get('asks', [])
                for i in range(10):
                    if i < len(asks):
                        row.extend([asks[i]['price'], asks[i]['vol']])
                    else:
                        row.extend([None, None]) # Fill with NULL if depth < 10

                # Add Bids 1~10
                bids = data.get('bids', [])
                for i in range(10):
                    if i < len(bids):
                        row.extend([bids[i]['price'], bids[i]['vol']])
                    else:
                        row.extend([None, None])

                # Total params: 3 + 20 + 20 = 43 params
                # Generate placeholders $1..$43
                placeholders = ",".join([f"${i+1}" for i in range(len(row))])
                
                query = f"""
                    INSERT INTO market_orderbook (
                        time, symbol, source,
                        ask_price1, ask_vol1, ask_price2, ask_vol2, ask_price3, ask_vol3, ask_price4, ask_vol4, ask_price5, ask_vol5,
                        ask_price6, ask_vol6, ask_price7, ask_vol7, ask_price8, ask_vol8, ask_price9, ask_vol9, ask_price10, ask_vol10,
                        bid_price1, bid_vol1, bid_price2, bid_vol2, bid_price3, bid_vol3, bid_price4, bid_vol4, bid_price5, bid_vol5,
                        bid_price6, bid_vol6, bid_price7, bid_vol7, bid_price8, bid_vol8, bid_price9, bid_vol9, bid_price10, bid_vol10
                    ) VALUES ({placeholders})
                """
                
                await conn.execute(query, *row)
            except Exception as e:
                logger.error(f"Orderbook Save Error: {e}")

    async def periodic_flush(self):
        """ÏßÄÏ†ïÎêú Í∏∞Í∞Ñ(FLUSH_INTERVAL)ÎßàÎã§ Ï£ºÍ∏∞Ï†ÅÏúºÎ°ú Î∞∞Ïπò Îç∞Ïù¥ÌÑ∞ Ï†ÅÏû¨"""
        while self.running:
            await asyncio.sleep(FLUSH_INTERVAL)
            if self.batch:
                await self.flush()

    async def flush(self):
        """Î©îÎ™®Î¶¨ Î≤ÑÌçºÏóê ÏåìÏù∏ Ìã± Îç∞Ïù¥ÌÑ∞Î•º DBÏóê Î≤åÌÅ¨ Ïù∏ÏÑúÌä∏"""
        if not self.batch:
            return

        async with self.db_pool.acquire() as conn:
            try:
                # asyncpg copy_records_to_table is fast
                await conn.copy_records_to_table(
                    'market_ticks',
                    records=self.batch,
                    columns=['time', 'symbol', 'price', 'volume', 'change', 'broker', 'broker_time', 'received_time', 'sequence_number', 'source']
                )
                logger.info(f"Flushed {len(self.batch)} ticks to TimescaleDB")

                # ISSUE-035: Record successful DB ingestion
                await self._record_db_success(len(self.batch))

                self.batch = []
            except Exception as e:
                logger.error(f"DB Flush Error: {e}")
                # In production, might want retry logic or DLQ

if __name__ == "__main__":
    archiver = TimescaleArchiver()
    asyncio.run(archiver.start())
